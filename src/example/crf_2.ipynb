{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md      data    records\tsetup_environment.sh\n",
      "conda_env.txt  models  run.sh\tsrc\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../../')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'./src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(5.3656), [2, 4, 2, 0])\n",
      "(tensor(5.3371), [6, 7, 4, 3])\n",
      "------------------------------\n",
      "[[3844, 6407, 3144, 2945], [5381, 3211, 1062, 1385]]\n",
      "[[7, 7, 7, 7], [3, 4, 4, 4]]\n",
      "([tensor(5.3656), tensor(5.3371)], [[2, 4, 2, 0], [6, 7, 4, 3]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# Make up some training data\n",
    "training_data = [\n",
    "    (\n",
    "    \"测 试 数 据\".split(),\n",
    "    \"O O O O\".split()\n",
    "    ), \n",
    "    (\n",
    "    \"网 易 公 司\".split(),\n",
    "    \"B-ORG I-ORG I-ORG I-ORG\".split()\n",
    "    )\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\", cache_dir = './models/bert/')\n",
    "b_x = []\n",
    "b_y = []\n",
    "tag_encoder = TagEncoder()\n",
    "model_config = {\n",
    "    'base_model_name': 'bert-base-chinese',\n",
    "    'cache_dir': './models/bert/',\n",
    "    'hidden_size': 768,\n",
    "    'hidden_dropout_prob': 0.1,\n",
    "}\n",
    "model = BertCRFForNER(model_config)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x,y in training_data:\n",
    "        x = tokenizer.convert_tokens_to_ids(x)\n",
    "        y = tag_encoder.single_to_ids(y)\n",
    "        b_x.append(x)\n",
    "        b_y.append(y)\n",
    "        x = torch.tensor([x], dtype=torch.long)\n",
    "        print(model.single_forward(x))\n",
    "\n",
    "    print('-'*30)\n",
    "    print(b_x)\n",
    "    print(b_y)\n",
    "    print(model.batch_forward(torch.tensor(b_x, dtype=torch.long)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:35<00:00,  3.50s/it]\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.train()\n",
    "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
    "for epoch in tqdm(range(10)):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    b_x = []\n",
    "    b_y = []\n",
    "    model.zero_grad()\n",
    "    # Step 1. Remember that Pytorch accumulates gradients.\n",
    "    # We need to clear them out before each instance\n",
    "    \n",
    "    for sentence, tags in training_data:\n",
    "        x = tokenizer.convert_tokens_to_ids(sentence)\n",
    "        y = tag_encoder.single_to_ids(tags)\n",
    "        b_x.append(x)\n",
    "        b_y.append(y)\n",
    "    \n",
    "    # Step 2. Get our inputs ready for the network, that is,\n",
    "    # turn them into Tensors of word indices.\n",
    "    b_x = torch.tensor(b_x, dtype=torch.long)\n",
    "    atten_mask = torch.ones_like(b_x)\n",
    "    b_y = torch.tensor(b_y, dtype=torch.long)\n",
    "\n",
    "    # Step 3. Run our forward pass.\n",
    "    loss = model.batch_neg_log_likelihood(b_x, atten_mask, b_y)\n",
    "\n",
    "    # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "    # calling optimizer.step()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(22.5156), [7, 7, 7, 7])\n",
      "(tensor(22.5157), [7, 7, 7, 7])\n",
      "------------------------------\n",
      "[[3844, 6407, 3144, 2945], [5381, 3211, 1062, 1385]]\n",
      "[[7, 7, 7, 7], [3, 4, 4, 4]]\n",
      "([tensor(22.5156), tensor(22.5157)], [[7, 7, 7, 7], [7, 7, 7, 7]])\n"
     ]
    }
   ],
   "source": [
    "b_x = []\n",
    "b_y = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x,y in training_data:\n",
    "        x = tokenizer.convert_tokens_to_ids(x)\n",
    "        y = tag_encoder.single_to_ids(y)\n",
    "        b_x.append(x)\n",
    "        b_y.append(y)\n",
    "        x = torch.tensor([x], dtype=torch.long)\n",
    "        print(model.single_forward(x))\n",
    "\n",
    "    print('-'*30)\n",
    "    print(b_x)\n",
    "    print(b_y)\n",
    "    print(model.batch_forward(torch.tensor(b_x, dtype=torch.long)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
